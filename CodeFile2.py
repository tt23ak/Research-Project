# -*- coding: utf-8 -*-
"""tayaba.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15BAQRubJI5awEHTidNIdBEhAMV3Rsox2

# **Download the UCI Repo**
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
breast_cancer_wisconsin_original = fetch_ucirepo(id=15)

# data (as pandas dataframes)
X = breast_cancer_wisconsin_original.data.features
y = breast_cancer_wisconsin_original.data.targets

# metadata
print(breast_cancer_wisconsin_original.metadata)

# variable information
print(breast_cancer_wisconsin_original.variables)

import pandas as pd

# Merge features and target into a single dataframe
df = pd.concat([X, y], axis=1)

# Display the merged dataframe
print(df.head())

df.info()

import seaborn as sns
sns.countplot(x='Class', data=df)

import seaborn as sns
import matplotlib.pyplot as plt

# Define custom colors for benign and malignant classes
colors = ['#1f77b4', '#ff7f0e']  # You can choose any colors you prefer

# Create count plot with specific colors
sns.countplot(x='Class', data=df, palette=colors)

# Add custom legend with colors
plt.legend(['Benign (2)', 'Malignant (4)'], title='Class', loc='upper right')
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Count of Benign and Malignant Cases')

plt.show()

"""#  **Preprocessing**"""

# Fill missing values in the Bare_nuclei column with the median
df['Bare_nuclei'].fillna(df['Bare_nuclei'].median(), inplace=True)

from sklearn.model_selection import train_test_split
# Separate features and target variable
X = df.drop(columns='Class')
y = df['Class']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#  **Initialize models**"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

models = {
    'Logistic Regression': LogisticRegression(C=0.01),  # High regularization, low iterations
    'Decision Tree': DecisionTreeClassifier(max_depth=2, min_samples_split=10),  # Limited depth, high split min
    'Random Forest': RandomForestClassifier(n_estimators=5, max_depth=2),  # Fewer trees, limited depth
    'Support Vector Machine': SVC(kernel='linear', C=0.1),  # Linear kernel with high regularization
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=20)  # High k value for excessive smoothing
}

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report




# Step 3: Train, predict, and evaluate each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f'{model_name} Accuracy: {accuracy:.2f}')

    # Classification report
    print(f'{model_name} Classification Report:\n', classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'{model_name} Confusion Matrix')
    plt.show()

"""# **SMOTE**"""

from imblearn.over_sampling import SMOTE

# Step 3: Apply SMOTE to balance the training set
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# Check class distribution after SMOTE
print("Class distribution after SMOTE:")
print(y_balanced.value_counts())

X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Step 5: Train, predict, and evaluate each model on balanced data
for model_name, model in models.items():
    # Train the model on balanced data
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f'{model_name} Accuracy: {accuracy:.2f}')

    # Classification report
    print(f'{model_name} Classification Report:\n', classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'{model_name} Confusion Matrix')
    plt.show()

"""# **Kfold**"""

from sklearn.model_selection import cross_val_score
clf = KNeighborsClassifier(n_neighbors=20)
scores = cross_val_score(clf, X, y, cv=60)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

from sklearn.model_selection import cross_val_score
clf = SVC(kernel='linear', C=0.1)
scores = cross_val_score(clf, X, y, cv=60)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

from sklearn.model_selection import cross_val_score
clf = RandomForestClassifier(n_estimators=5, max_depth=2)
scores = cross_val_score(clf, X, y, cv=60)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

from sklearn.model_selection import cross_val_score
clf = DecisionTreeClassifier(max_depth=2, min_samples_split=10)
scores = cross_val_score(clf, X, y, cv=60)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

from sklearn.model_selection import cross_val_score
clf = LogisticRegression(C=0.01)
scores = cross_val_score(clf, X, y, cv=60)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))